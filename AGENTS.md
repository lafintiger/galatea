# Galatea - AI Agent Onboarding Guide

> This document is for AI agents continuing development on this project. Read this first to understand the codebase, architecture, and roadmap.

## ğŸ¯ Project Overview

**Galatea** is a local voice AI companion - think Alexa/Siri but running entirely on the user's hardware with no cloud dependencies. Named after the Greek myth of Pygmalion and Galatea.

### Core Value Proposition
- **100% Local**: All processing happens on user's machine (privacy-focused)
- **Real-time Voice Chat**: Seamless conversation with low latency
- **Customizable**: User can choose LLM models, voices, and personality
- **Extensible**: Architecture designed for future integrations (vision, tools, memory)

### Current Status: Phase 3 âœ…
- Voice input (STT) via Faster-Whisper
- LLM chat via Ollama
- Voice output (TTS) via **Piper** (fast) or **Kokoro** (HD quality)
- Sentence-level streaming for low latency
- Push-to-talk and Open Mic (VAD) modes
- Settings persistence
- **Time Awareness** - Gala knows time of day, weekends, holidays
- **Keyboard Shortcuts** - Spacebar for PTT, Escape to interrupt
- **Clean Interruption** - Instantly stops audio, clears queue
- **Export Conversations** - Save as Markdown, Text, or JSON
- **Enhanced Status Bar** - Shows model info, TTS provider, retry button
- **Conversation History** - Save/load past conversations with rename/delete
- **Web Search** - Search via SearXNG or Perplexica, natural language triggers
- **RAG System** - Background embedding with LanceDB + Ollama (Qwen3-Embedding-8B)

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         FRONTEND                                 â”‚
â”‚  React + TypeScript + Vite + Tailwind CSS                       â”‚
â”‚  Port: 5173 (dev)                                               â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ VoiceInterface â”‚ â”‚  Settings   â”‚  â”‚  useWebSocket (hook)    â”‚ â”‚
â”‚  â”‚ (mic, visual) â”‚ â”‚  (config)   â”‚  â”‚  (real-time comms)      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                                        â”‚               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ WebSocket â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         BACKEND                                  â”‚
â”‚  FastAPI + Python + WebSockets                                  â”‚
â”‚  Port: 8000                                                     â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ main.py     â”‚  â”‚ ollama.py   â”‚  â”‚  wyoming.py             â”‚ â”‚
â”‚  â”‚ (WS handler)â”‚  â”‚ (LLM client)â”‚  â”‚  (STT/TTS clients)      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                â”‚                      â”‚               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚web_search.pyâ”‚  â”‚conversation â”‚  â”‚  RAG Services           â”‚ â”‚
â”‚  â”‚(SearXNG/    â”‚  â”‚_history.py  â”‚  â”‚  embedding.py (LanceDB) â”‚ â”‚
â”‚  â”‚ Perplexica) â”‚  â”‚(save/load)  â”‚  â”‚  model_manager.py       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  background_worker.py   â”‚ â”‚
â”‚         â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                                       â”‚
          â–¼                                       â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Ollama  â”‚    â”‚ SearXNG  â”‚    â”‚   Wyoming Whisper (STT)  â”‚
    â”‚  (LLM)   â”‚    â”‚ (search) â”‚    â”‚   Docker: :10300         â”‚
    â”‚ :11434   â”‚    â”‚ :4000    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   Wyoming Piper (TTS)    â”‚
                    â”‚Perplexicaâ”‚    â”‚   Docker: :10200         â”‚
                    â”‚(AI srch) â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ :3000    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   Kokoro (HD TTS)        â”‚
                                    â”‚   Docker: :8880          â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Docker Services Required
```bash
# Whisper (STT) - Wyoming protocol
docker run -d --name wyoming-whisper \
  -p 10300:10300 \
  rhasspy/wyoming-whisper --model small --language en

# Piper (TTS) - Wyoming protocol (Fast, CPU-friendly)
docker run -d --name piper \
  -p 10200:10200 \
  -v /path/to/voices:/config \
  lscr.io/linuxserver/piper

# Kokoro (TTS) - OpenAI-compatible API (HD quality)
docker run -d --name kokoro-tts \
  -p 8880:8880 \
  ghcr.io/remsky/kokoro-fastapi-cpu:latest

# SearXNG (Web Search) - Meta-search engine
docker run -d --name searxng \
  -p 4000:8080 \
  -v ./searxng:/etc/searxng \
  searxng/searxng

# Perplexica (AI Search) - Optional, AI-powered search with summaries
# See https://github.com/ItzCrazyKns/Perplexica for setup
```

**TTS Options:**
- **Piper**: Fast, lightweight, good for CPU. Wyoming protocol.
- **Kokoro**: Higher quality, more natural. OpenAI-compatible API. User can switch in Settings.

**Web Search Options:**
- **SearXNG**: Privacy-focused meta-search. Fast, aggregates from multiple engines.
- **Perplexica**: AI-powered search with built-in summaries. Uses Ollama for AI.

---

## ğŸ“ Codebase Structure

```
galatea/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI app, WebSocket handler, core logic
â”‚   â”‚   â”œâ”€â”€ config.py            # Pydantic settings (hosts, ports, defaults)
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ ollama.py        # Ollama LLM client with streaming + time awareness
â”‚   â”‚   â”‚   â”œâ”€â”€ wyoming.py       # Wyoming protocol clients (Whisper/Piper)
â”‚   â”‚   â”‚   â”œâ”€â”€ kokoro.py        # Kokoro TTS client (OpenAI-compatible API)
â”‚   â”‚   â”‚   â”œâ”€â”€ web_search.py    # SearXNG/Perplexica search integration
â”‚   â”‚   â”‚   â”œâ”€â”€ conversation_history.py  # Save/load conversations
â”‚   â”‚   â”‚   â”œâ”€â”€ settings_manager.py  # User settings persistence
â”‚   â”‚   â”‚   â”œâ”€â”€ embedding.py     # LanceDB vector embeddings via Ollama
â”‚   â”‚   â”‚   â”œâ”€â”€ model_manager.py # Ollama model load/unload for VRAM
â”‚   â”‚   â”‚   â””â”€â”€ background_worker.py  # Background embedding processor
â”‚   â”‚   â””â”€â”€ models/
â”‚   â”‚       â””â”€â”€ schemas.py       # Pydantic models (UserSettings, etc.)
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx              # Main app layout
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ VoiceInterface.tsx   # Mic button, status, visualizer, search
â”‚   â”‚   â”‚   â”œâ”€â”€ Settings.tsx         # Settings panel
â”‚   â”‚   â”‚   â”œâ”€â”€ AudioVisualizer.tsx  # Canvas-based audio viz
â”‚   â”‚   â”‚   â”œâ”€â”€ Transcript.tsx       # Conversation display + export
â”‚   â”‚   â”‚   â”œâ”€â”€ StatusBar.tsx        # Connection status
â”‚   â”‚   â”‚   â””â”€â”€ HistoryPanel.tsx     # Conversation history sidebar
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â”œâ”€â”€ useWebSocket.ts      # WebSocket + audio queue
â”‚   â”‚   â”‚   â””â”€â”€ useAudioRecorder.ts  # Mic recording + VAD
â”‚   â”‚   â”œâ”€â”€ stores/
â”‚   â”‚   â”‚   â”œâ”€â”€ settingsStore.ts     # Zustand store for settings
â”‚   â”‚   â”‚   â””â”€â”€ conversationStore.ts # Zustand store for chat state
â”‚   â”‚   â””â”€â”€ styles/
â”‚   â”‚       â””â”€â”€ index.css            # Tailwind + custom cyber theme
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.ts           # Proxy config for dev
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ download_voices.py       # Script to download Piper voices
â”‚
â”œâ”€â”€ PRD.md                       # Product requirements document
â”œâ”€â”€ README.md                    # Setup instructions
â””â”€â”€ AGENTS.md                    # This file
```

---

## ğŸ”‘ Key Implementation Details

### 1. WebSocket Message Flow

**Frontend â†’ Backend:**
```json
{"type": "audio_data", "audio": "<base64 wav>"}
{"type": "text_message", "content": "Hello Gala"}
{"type": "web_search", "query": "RTX 5090 specs", "provider": "auto"}
{"type": "interrupt"}
{"type": "settings_update", "settings": {...}}
{"type": "clear_history"}
```

**Backend â†’ Frontend:**
```json
{"type": "status", "state": "idle|processing|thinking|speaking|searching"}
{"type": "transcription", "text": "...", "final": true}
{"type": "llm_chunk", "text": "..."}
{"type": "llm_complete", "text": "..."}
{"type": "audio_chunk", "audio": "<base64 wav>", "sentence": "..."}
{"type": "search_start", "query": "..."}
{"type": "search_results", "data": {...}}
{"type": "error", "message": "..."}
```

### 2. Sentence-Level TTS Streaming

To reduce latency, we don't wait for the full LLM response. Instead:
1. Buffer LLM chunks
2. When a sentence boundary is detected (`. ! ?`), send that sentence to Piper
3. Stream audio chunks to frontend immediately
4. Frontend queues and plays audio sequentially

See `generate_response()` in `backend/app/main.py`.

### 3. Voice Activity Detection (VAD)

In `useAudioRecorder.ts`:
- `startVAD()` - Starts continuous mic listening
- Detects speech start when audio level > `VAD_SPEECH_THRESHOLD`
- Detects speech end after `VAD_SILENCE_DURATION` ms of silence
- Automatically converts and sends audio when speech ends

### 4. Text Cleaning for TTS

The `clean_for_speech()` function in `main.py` removes:
- Emojis and Unicode symbols
- Action markers: `*smiles*`, `(laughs)`, `[nods]`
- `<think>` blocks from thinking models
- Markdown formatting

### 5. Thinking Model Handling

For models like Qwen3 that have chain-of-thought:
- System prompt includes `/no_think` instruction
- User messages get `/no_think` appended
- `<think>` blocks are filtered from stream before display

### 6. Time Awareness

The `get_time_context()` function in `ollama.py` provides:
- Time of day (morning/afternoon/evening/night)
- Day of week (weekday vs weekend)
- Holiday detection (major US holidays)
- Greeting suggestions for the LLM

### 7. Interruption System

When user presses Escape or clicks stop:
1. Frontend: `stopAllAudio()` immediately stops playback
2. Frontend: Clears audio queue (`audioQueueRef.current = []`)
3. Backend: Sets `should_interrupt = True` 
4. Backend: Stops TTS generation for remaining sentences
5. Both: Reset to idle state

### 8. Keyboard Shortcuts

| Key | Action | Condition |
|-----|--------|-----------|
| **Spacebar** (hold) | Push-to-talk record | In PTT mode, idle state |
| **Spacebar** (release) | Stop recording & send | Recording active |
| **Escape** | Interrupt Gala | Speaking/processing |

Shortcuts are disabled when typing in text input.

### 9. Web Search Integration

Gala can search the web using SearXNG or Perplexica. Search can be triggered:

**Via Voice/Text (Natural Language):**
```
"Search for RTX 5090 specs"
"Look up the weather in Paris"
"Find out about quantum computing"
"Google best pizza in NYC"
"What is the latest AI news?"
```

**Via Search Button:**
- Click the ğŸ” button next to the text input
- Enter query in the popup dialog
- Results are summarized by Gala

**Architecture:**
```
User says "search for X" â†’ detect_search_intent() â†’ web_search.py
                                                         â†“
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚  SearXNG (port 4000)                â”‚
                              â”‚  - Fast meta-search                 â”‚
                              â”‚  - Aggregates Google/Bing/DDG       â”‚
                              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                              â”‚  Perplexica (port 3000) - Optional  â”‚
                              â”‚  - AI-powered search                â”‚
                              â”‚  - Built-in summaries via Ollama    â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â†“
                              Results formatted â†’ LLM summarizes â†’ TTS speaks
```

**Search Trigger Phrases:**
| Pattern | Example |
|---------|---------|
| `search for...` | "Search for electric cars" |
| `look up...` | "Look up Python tutorials" |
| `find out about...` | "Find out about quantum computing" |
| `google...` | "Google best restaurants" |
| `check the...` | "Check the weather" |

**Auto-Search Topics** (always triggers search):
| Topic | Examples |
|-------|----------|
| **Weather** | "What's the weather?", "Will it rain tomorrow?", "Check the forecast" |
| **News** | "What's happening in tech?", "Latest news on AI", "Recent developments" |
| **Prices** | "How much does iPhone cost?", "Bitcoin price", "Stock price of Apple" |
| **Sports** | "Who won the game?", "NBA standings", "Match score" |
| **Schedules** | "When does the store open?", "Release date of...", "Hours of..." |
| **Products** | "RTX 5090 specs", "Best laptop for gaming", "iPhone 16 reviews" |
| **Movies** | "What movies are playing?", "Show times tonight" |
| **Location** | "Restaurants nearby", "Directions to...", "Phone number for..." |

**System Prompt Integration**: Gala's system prompt instructs her to search when she doesn't know something. She'll say "Let me look that up" and the system will automatically perform the search.

### 10. Conversation History

Save and load past conversations via the History panel (ğŸ• button in header).

**Features:**
- **Save Current** - Saves current conversation with auto-generated title
- **New** - Clears current conversation and starts fresh
- **Load** - Click any saved conversation to restore it
- **Rename** - Click edit icon to rename a conversation
- **Delete** - Click trash icon to remove a conversation

**Storage:** Conversations saved as JSON in `backend/data/conversations/`

**API Endpoints:**
| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/api/conversations` | List all saved conversations |
| GET | `/api/conversations/{id}` | Get specific conversation |
| POST | `/api/conversations` | Save new/update existing |
| DELETE | `/api/conversations/{id}` | Delete conversation |
| PATCH | `/api/conversations/{id}` | Rename conversation |

### 11. RAG System (Background Embeddings)

Gala uses LanceDB + Ollama embeddings for semantic memory. Embeddings are processed **in the background** to avoid interrupting conversation flow.

**Architecture:**
```
User saves conversation â†’ JSON stored immediately
                            â†“
                       Added to embedding queue
                            â†“
                       (User is idle for 5 minutes)
                            â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Background Worker                  â”‚
        â”‚  1. Unload chat model (free VRAM)   â”‚
        â”‚  2. Load embedding model (5.4GB)    â”‚
        â”‚  3. Embed all pending conversations â”‚
        â”‚  4. Store vectors in LanceDB        â”‚
        â”‚  5. Unload embedding model          â”‚
        â”‚  6. Reload chat model               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
        User asks question â†’ RAG retrieves similar context
                            â†“
        Context injected into system prompt â†’ Better answers!
```

**Components:**

| File | Purpose |
|------|---------|
| `embedding.py` | LanceDB storage + Ollama embedding API calls |
| `model_manager.py` | Load/unload Ollama models for VRAM management |
| `background_worker.py` | Idle detection + batch processing |

**Configuration:**
- **Embedding Model**: `ZimaBlueAI/Qwen3-Embedding-8B:Q5_K_M` (5.4GB, high quality)
- **Idle Timeout**: 5 minutes of no activity before processing
- **Vector Dimensions**: 4096 (Qwen3-Embedding output)

**API Endpoints:**
| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/api/rag/status` | Worker status, pending count, embedding stats |
| POST | `/api/rag/process` | Manually trigger embedding (bypass idle wait) |
| GET | `/api/rag/search?query=...` | Search the knowledge base directly |

**SanctumWriter Compatibility:**
This RAG implementation uses the same stack as SanctumWriter:
- **LanceDB** for vector storage
- **Ollama embeddings** API
- Same embedding model options

Future integration will allow shared memory between Gala and SanctumWriter.

---

## ğŸ¨ UI/UX Decisions

- **Futuristic Cyberpunk Theme**: Cyan accents, dark backgrounds, glow effects
- **Minimal Text**: Voice-first interface, transcript is optional
- **Visual Feedback**: 
  - Audio visualizer shows mic levels
  - Color-coded states (green=listening, yellow=recording, cyan=speaking)
  - Pulse animations for active states
  - Emoji status indicators (â— ğŸ™ï¸ ğŸ§  ğŸ—£ï¸)

### Status Bar Features
- Connection status with retry button on error
- Model name + size (e.g., "qwen3-abliterated (5.0GB)")
- Voice name + TTS provider badge (Fast/HD)
- Dismissible error messages

### Transcript Features
- Auto-scrolling conversation view
- Export dropdown (Markdown, Text, JSON)
- Clear conversation button
- User/Assistant message styling with timestamps

---

## ğŸ› ï¸ Development Workflow

### Start Services
```bash
# Terminal 1: Backend
cd backend
.\venv\Scripts\activate  # Windows
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Terminal 2: Frontend
cd frontend
npm run dev

# Required Docker containers must be running
docker start wyoming-whisper piper
```

### Key Files to Edit

| Feature | Files |
|---------|-------|
| LLM behavior | `backend/app/services/ollama.py` (system prompt) |
| Voice settings | `backend/app/models/schemas.py` (UserSettings) |
| WebSocket logic | `backend/app/main.py` |
| Web search | `backend/app/services/web_search.py` |
| Conversation history | `backend/app/services/conversation_history.py` |
| UI components | `frontend/src/components/*.tsx` |
| State management | `frontend/src/stores/*.ts` |
| Audio handling | `frontend/src/hooks/useAudioRecorder.ts` |
| Search config | `backend/app/config.py` (searxng/perplexica hosts)

---

## ğŸ—ºï¸ Roadmap

### âœ… Completed (December 2024)

| Feature | Description |
|---------|-------------|
| **Kokoro TTS** | High-quality TTS option alongside Piper |
| **Time Awareness** | Gala knows time of day, day of week, holidays, weekends |
| **Better Interruption** | Escape key stops audio instantly, clears queue |
| **Keyboard Shortcuts** | Spacebar for push-to-talk, Escape to interrupt |
| **Export Conversation** | Download as Markdown, Plain Text, or JSON |
| **Clear Conversation** | Trash button in transcript |
| **Better Status Indicators** | Emoji icons for each state (ğŸ™ï¸ğŸ§ ğŸ—£ï¸ğŸ”) |
| **Enhanced Status Bar** | Model size, TTS provider badge, retry button |
| **Conversation History** | Save/load past conversations with rename/delete |
| **Web Search** | SearXNG + Perplexica integration with natural language triggers |
| **RAG System** | LanceDB + Ollama embeddings with background processing |

### ğŸ“‹ Phase 4: Future Features

| Feature | Description | Complexity |
|---------|-------------|------------|
| **Multiple Personas** | Switch between Gala "personalities" | Medium |
| **Vision** | Describe images/screenshots | High |
| **Tool Calling** | File ops, code execution, smart home | High |
| **Multi-language** | Support other languages for voice | Medium |
| **Emotion Detection** | Analyze user sentiment | High |

**Not Planned** (per user preference):
- Wake Word ("Hey Gala") - Privacy concern, user prefers manual activation

---

## âš ï¸ Known Issues & Gotchas

1. **Thinking Models**: Qwen3 and similar need `/no_think` - already handled
2. **Microphone Permissions**: Browser must grant access, show clear error
3. **Wyoming Protocol**: Use official `wyoming` package, not custom implementation
4. **Piper Voice Location**: LinuxServer Piper container uses `/config/` not `/data/`
5. **Audio Playback**: Must handle browser autoplay policies (AudioContext resume)

---

## ğŸ’¡ Tips for Agents

1. **Read the PRD.md** for full feature requirements
2. **Check schemas.py** before adding new settings
3. **Test with Open Mic mode** - it's more complex than push-to-talk
4. **Backend auto-reloads** with `--reload` flag
5. **Frontend HMR** - but sometimes needs full refresh
6. **User has 5090 GPU** with 24GB VRAM - can run large models

---

## ğŸ“ User Preferences

- **Name**: User prefers "Gala" as nickname for the assistant
- **Voice**: Prefers Kokoro TTS (HD quality) - `af_heart` voice
- **Model**: Using Qwen3 abliterated (needs thinking disabled for speed)
- **Style**: Prefers conversational, natural responses
- **Clean Speech**: No emojis, no action markers, no thinking aloud
- **Privacy**: No always-on listening, no wake word - user controls when mic is active
- **Hardware**: RTX 5090 (24GB VRAM), powerful CPU with NPU

---

## ğŸ”— Related Projects

- **SanctumWriter** (`local-doc-editor/`): Local AI writing app
  - Uses same LanceDB + Ollama embeddings for RAG
  - Future integration planned: shared memory between Gala and SanctumWriter

---

*Last updated: December 8, 2024*
*Phase: 2 (Conversation History + Web Search)*
*Repository: https://github.com/lafintiger/galatea*



